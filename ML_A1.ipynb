{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9BVagyo6In0fb3RT/URVf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"12ccc5e1"},"source":["\n","#Madhura Vilas Suroshe\n","#24102C2002\n","# Task\n","Perform a comprehensive house price prediction analysis using the \"housing.csv\" dataset. This analysis should include data preprocessing (handling missing values, encoding categorical features, and scaling numerical features), splitting the data into training and testing sets, and training and evaluating three different regression models: Linear Regression, Ridge Regression, and Decision Tree Regressor. For each model, calculate and report RMSE (Train), RMSE (Test), and MAE (Test). Conclude the analysis by comparing the models' performances, discussing instances of underfitting and overfitting, commenting on at least one relevant real-world machine learning issue, and providing a comprehensive summary of the findings."]},{"cell_type":"code","source":[],"metadata":{"id":"zenmnqmEA8xM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2b8a6490"},"source":["Load dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a82a6242","executionInfo":{"status":"ok","timestamp":1769005048044,"user_tz":-330,"elapsed":553,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"438c791d-2502-42e2-e344-ed1d16730498"},"source":["import requests\n","\n","# URL of the raw housing.csv file on GitHub\n","url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv'\n","\n","# Define the local filename\n","filename = 'housing.csv'\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Check if the request was successful (status code 200)\n","if response.status_code == 200:\n","    # Write the content to a local file\n","    with open(filename, 'wb') as f:\n","        f.write(response.content)\n","    print(f\"'{filename}' downloaded successfully.\")\n","else:\n","    print(f\"Failed to download '{filename}'. Status code: {response.status_code}\")\n","\n","\n","import pandas as pd\n","\n","df = pd.read_csv('housing.csv')\n","\n","print(\"First 5 rows of the DataFrame:\")\n","print(df.head())\n","\n","print(\"\\nConcise summary of the DataFrame (info()):\")\n","df.info()\n","\n","print(\"\\nDescriptive statistics of the DataFrame (describe()):\")\n","print(df.describe())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'housing.csv' downloaded successfully.\n","First 5 rows of the DataFrame:\n","   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0    -122.23     37.88                41.0        880.0           129.0   \n","1    -122.22     37.86                21.0       7099.0          1106.0   \n","2    -122.24     37.85                52.0       1467.0           190.0   \n","3    -122.25     37.85                52.0       1274.0           235.0   \n","4    -122.25     37.85                52.0       1627.0           280.0   \n","\n","   population  households  median_income  median_house_value ocean_proximity  \n","0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n","1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n","2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n","3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n","4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n","\n","Concise summary of the DataFrame (info()):\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20640 entries, 0 to 20639\n","Data columns (total 10 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   longitude           20640 non-null  float64\n"," 1   latitude            20640 non-null  float64\n"," 2   housing_median_age  20640 non-null  float64\n"," 3   total_rooms         20640 non-null  float64\n"," 4   total_bedrooms      20433 non-null  float64\n"," 5   population          20640 non-null  float64\n"," 6   households          20640 non-null  float64\n"," 7   median_income       20640 non-null  float64\n"," 8   median_house_value  20640 non-null  float64\n"," 9   ocean_proximity     20640 non-null  object \n","dtypes: float64(9), object(1)\n","memory usage: 1.6+ MB\n","\n","Descriptive statistics of the DataFrame (describe()):\n","          longitude      latitude  housing_median_age   total_rooms  \\\n","count  20640.000000  20640.000000        20640.000000  20640.000000   \n","mean    -119.569704     35.631861           28.639486   2635.763081   \n","std        2.003532      2.135952           12.585558   2181.615252   \n","min     -124.350000     32.540000            1.000000      2.000000   \n","25%     -121.800000     33.930000           18.000000   1447.750000   \n","50%     -118.490000     34.260000           29.000000   2127.000000   \n","75%     -118.010000     37.710000           37.000000   3148.000000   \n","max     -114.310000     41.950000           52.000000  39320.000000   \n","\n","       total_bedrooms    population    households  median_income  \\\n","count    20433.000000  20640.000000  20640.000000   20640.000000   \n","mean       537.870553   1425.476744    499.539680       3.870671   \n","std        421.385070   1132.462122    382.329753       1.899822   \n","min          1.000000      3.000000      1.000000       0.499900   \n","25%        296.000000    787.000000    280.000000       2.563400   \n","50%        435.000000   1166.000000    409.000000       3.534800   \n","75%        647.000000   1725.000000    605.000000       4.743250   \n","max       6445.000000  35682.000000   6082.000000      15.000100   \n","\n","       median_house_value  \n","count        20640.000000  \n","mean        206855.816909  \n","std         115395.615874  \n","min          14999.000000  \n","25%         119600.000000  \n","50%         179700.000000  \n","75%         264725.000000  \n","max         500001.000000  \n"]}]},{"cell_type":"markdown","metadata":{"id":"20b7deab"},"source":["## Handle Missing Values and Categorical Features"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"706ac21f","executionInfo":{"status":"ok","timestamp":1769005074565,"user_tz":-330,"elapsed":59,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"884cae31-e4cb-44c7-d9c4-44715cb897a6"},"source":["print(\"Missing values before handling:\")\n","print(df.isnull().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values before handling:\n","longitude               0\n","latitude                0\n","housing_median_age      0\n","total_rooms             0\n","total_bedrooms        207\n","population              0\n","households              0\n","median_income           0\n","median_house_value      0\n","ocean_proximity         0\n","dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"972662df"},"source":["Handling missing values"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"850de7fc","executionInfo":{"status":"ok","timestamp":1769005083619,"user_tz":-330,"elapsed":90,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"2b9e33d8-8ca3-445b-c6c5-e9ac16c045e6"},"source":["median_total_bedrooms = df['total_bedrooms'].median()\n","df['total_bedrooms'].fillna(median_total_bedrooms, inplace=True)\n","\n","print(\"Missing values after filling 'total_bedrooms':\")\n","print(df.isnull().sum())\n","\n","# Identify categorical features\n","categorical_features = df.select_dtypes(include=['object']).columns\n","print(f\"\\nCategorical features identified: {list(categorical_features)}\")\n","\n","# Apply one-hot encoding to 'ocean_proximity'\n","df = pd.get_dummies(df, columns=['ocean_proximity'], drop_first=False)\n","\n","print(\"\\nDataFrame after one-hot encoding (first 5 rows):\")\n","print(df.head())\n","\n","print(\"\\nConcise summary of the DataFrame after preprocessing (info()):\")\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values after filling 'total_bedrooms':\n","longitude             0\n","latitude              0\n","housing_median_age    0\n","total_rooms           0\n","total_bedrooms        0\n","population            0\n","households            0\n","median_income         0\n","median_house_value    0\n","ocean_proximity       0\n","dtype: int64\n","\n","Categorical features identified: ['ocean_proximity']\n","\n","DataFrame after one-hot encoding (first 5 rows):\n","   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0    -122.23     37.88                41.0        880.0           129.0   \n","1    -122.22     37.86                21.0       7099.0          1106.0   \n","2    -122.24     37.85                52.0       1467.0           190.0   \n","3    -122.25     37.85                52.0       1274.0           235.0   \n","4    -122.25     37.85                52.0       1627.0           280.0   \n","\n","   population  households  median_income  median_house_value  \\\n","0       322.0       126.0         8.3252            452600.0   \n","1      2401.0      1138.0         8.3014            358500.0   \n","2       496.0       177.0         7.2574            352100.0   \n","3       558.0       219.0         5.6431            341300.0   \n","4       565.0       259.0         3.8462            342200.0   \n","\n","   ocean_proximity_<1H OCEAN  ocean_proximity_INLAND  ocean_proximity_ISLAND  \\\n","0                      False                   False                   False   \n","1                      False                   False                   False   \n","2                      False                   False                   False   \n","3                      False                   False                   False   \n","4                      False                   False                   False   \n","\n","   ocean_proximity_NEAR BAY  ocean_proximity_NEAR OCEAN  \n","0                      True                       False  \n","1                      True                       False  \n","2                      True                       False  \n","3                      True                       False  \n","4                      True                       False  \n","\n","Concise summary of the DataFrame after preprocessing (info()):\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20640 entries, 0 to 20639\n","Data columns (total 14 columns):\n"," #   Column                      Non-Null Count  Dtype  \n","---  ------                      --------------  -----  \n"," 0   longitude                   20640 non-null  float64\n"," 1   latitude                    20640 non-null  float64\n"," 2   housing_median_age          20640 non-null  float64\n"," 3   total_rooms                 20640 non-null  float64\n"," 4   total_bedrooms              20640 non-null  float64\n"," 5   population                  20640 non-null  float64\n"," 6   households                  20640 non-null  float64\n"," 7   median_income               20640 non-null  float64\n"," 8   median_house_value          20640 non-null  float64\n"," 9   ocean_proximity_<1H OCEAN   20640 non-null  bool   \n"," 10  ocean_proximity_INLAND      20640 non-null  bool   \n"," 11  ocean_proximity_ISLAND      20640 non-null  bool   \n"," 12  ocean_proximity_NEAR BAY    20640 non-null  bool   \n"," 13  ocean_proximity_NEAR OCEAN  20640 non-null  bool   \n","dtypes: bool(5), float64(9)\n","memory usage: 1.5 MB\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1739886316.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['total_bedrooms'].fillna(median_total_bedrooms, inplace=True)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b763b21e","executionInfo":{"status":"ok","timestamp":1769005199176,"user_tz":-330,"elapsed":90,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"71117c35-546d-4b1b-ec03-59c3794b64bb"},"source":["median_total_bedrooms = df['total_bedrooms'].median()\n","df['total_bedrooms'] = df['total_bedrooms'].fillna(median_total_bedrooms)\n","\n","print(\"Missing values after filling 'total_bedrooms':\")\n","print(df.isnull().sum())\n","\n","# Identify categorical features\n","categorical_features = df.select_dtypes(include=['object']).columns\n","print(f\"\\nCategorical features identified: {list(categorical_features)}\")\n","\n","# Apply one-hot encoding to 'ocean_proximity' if it still exists\n","if 'ocean_proximity' in df.columns:\n","    df = pd.get_dummies(df, columns=['ocean_proximity'], drop_first=False)\n","    print(\"\\n'ocean_proximity' column successfully one-hot encoded.\")\n","else:\n","    print(\"\\n'ocean_proximity' column not found, likely already one-hot encoded in a previous step.\")\n","\n","print(\"\\nDataFrame after preprocessing (first 5 rows):\")\n","print(df.head())\n","\n","print(\"\\nConcise summary of the DataFrame after preprocessing (info()):\")\n","df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values after filling 'total_bedrooms':\n","longitude                     0\n","latitude                      0\n","housing_median_age            0\n","total_rooms                   0\n","total_bedrooms                0\n","population                    0\n","households                    0\n","median_income                 0\n","median_house_value            0\n","ocean_proximity_<1H OCEAN     0\n","ocean_proximity_INLAND        0\n","ocean_proximity_ISLAND        0\n","ocean_proximity_NEAR BAY      0\n","ocean_proximity_NEAR OCEAN    0\n","dtype: int64\n","\n","Categorical features identified: []\n","\n","'ocean_proximity' column not found, likely already one-hot encoded in a previous step.\n","\n","DataFrame after preprocessing (first 5 rows):\n","   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0    -122.23     37.88                41.0        880.0           129.0   \n","1    -122.22     37.86                21.0       7099.0          1106.0   \n","2    -122.24     37.85                52.0       1467.0           190.0   \n","3    -122.25     37.85                52.0       1274.0           235.0   \n","4    -122.25     37.85                52.0       1627.0           280.0   \n","\n","   population  households  median_income  median_house_value  \\\n","0       322.0       126.0         8.3252            452600.0   \n","1      2401.0      1138.0         8.3014            358500.0   \n","2       496.0       177.0         7.2574            352100.0   \n","3       558.0       219.0         5.6431            341300.0   \n","4       565.0       259.0         3.8462            342200.0   \n","\n","   ocean_proximity_<1H OCEAN  ocean_proximity_INLAND  ocean_proximity_ISLAND  \\\n","0                      False                   False                   False   \n","1                      False                   False                   False   \n","2                      False                   False                   False   \n","3                      False                   False                   False   \n","4                      False                   False                   False   \n","\n","   ocean_proximity_NEAR BAY  ocean_proximity_NEAR OCEAN  \n","0                      True                       False  \n","1                      True                       False  \n","2                      True                       False  \n","3                      True                       False  \n","4                      True                       False  \n","\n","Concise summary of the DataFrame after preprocessing (info()):\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20640 entries, 0 to 20639\n","Data columns (total 14 columns):\n"," #   Column                      Non-Null Count  Dtype  \n","---  ------                      --------------  -----  \n"," 0   longitude                   20640 non-null  float64\n"," 1   latitude                    20640 non-null  float64\n"," 2   housing_median_age          20640 non-null  float64\n"," 3   total_rooms                 20640 non-null  float64\n"," 4   total_bedrooms              20640 non-null  float64\n"," 5   population                  20640 non-null  float64\n"," 6   households                  20640 non-null  float64\n"," 7   median_income               20640 non-null  float64\n"," 8   median_house_value          20640 non-null  float64\n"," 9   ocean_proximity_<1H OCEAN   20640 non-null  bool   \n"," 10  ocean_proximity_INLAND      20640 non-null  bool   \n"," 11  ocean_proximity_ISLAND      20640 non-null  bool   \n"," 12  ocean_proximity_NEAR BAY    20640 non-null  bool   \n"," 13  ocean_proximity_NEAR OCEAN  20640 non-null  bool   \n","dtypes: bool(5), float64(9)\n","memory usage: 1.5 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"46bb9c12"},"source":["## Feature Scaling"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8cc7047","executionInfo":{"status":"ok","timestamp":1769005224345,"user_tz":-330,"elapsed":1917,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"41bb693d-4fb2-41e0-9e69-a0fcb0e08469"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","# 1. Separate the target variable `median_house_value` from the features\n","X = df.drop('median_house_value', axis=1)\n","y = df['median_house_value']\n","\n","# 2. Identify all numerical feature columns in `X` that are not boolean\n","numerical_features = X.select_dtypes(include=['float64', 'int64']).columns\n","\n","# 3. Import StandardScaler and 4. Instantiate a StandardScaler object\n","scaler = StandardScaler()\n","\n","# 5. Fit the StandardScaler to the identified numerical features in `X` and then transform these features\n","X[numerical_features] = scaler.fit_transform(X[numerical_features])\n","\n","# 6. Print the first few rows of the updated `X` DataFrame to verify the scaling\n","print(\"First 5 rows of X after feature scaling:\")\n","print(X.head())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 rows of X after feature scaling:\n","   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n","0  -1.327835  1.052548            0.982143    -0.804819       -0.972476   \n","1  -1.322844  1.043185           -0.607019     2.045890        1.357143   \n","2  -1.332827  1.038503            1.856182    -0.535746       -0.827024   \n","3  -1.337818  1.038503            1.856182    -0.624215       -0.719723   \n","4  -1.337818  1.038503            1.856182    -0.462404       -0.612423   \n","\n","   population  households  median_income  ocean_proximity_<1H OCEAN  \\\n","0   -0.974429   -0.977033       2.344766                      False   \n","1    0.861439    1.669961       2.332238                      False   \n","2   -0.820777   -0.843637       1.782699                      False   \n","3   -0.766028   -0.733781       0.932968                      False   \n","4   -0.759847   -0.629157      -0.012881                      False   \n","\n","   ocean_proximity_INLAND  ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n","0                   False                   False                      True   \n","1                   False                   False                      True   \n","2                   False                   False                      True   \n","3                   False                   False                      True   \n","4                   False                   False                      True   \n","\n","   ocean_proximity_NEAR OCEAN  \n","0                       False  \n","1                       False  \n","2                       False  \n","3                       False  \n","4                       False  \n"]}]},{"cell_type":"markdown","metadata":{"id":"7c2fca50"},"source":["## Split Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a0ec2ba","executionInfo":{"status":"ok","timestamp":1769005246976,"user_tz":-330,"elapsed":477,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"aa91978b-d07e-44c3-8c2a-b8d6e3174340"},"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Shape of X_train:\", X_train.shape)\n","print(\"Shape of X_test:\", X_test.shape)\n","print(\"Shape of y_train:\", y_train.shape)\n","print(\"Shape of y_test:\", y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X_train: (16512, 13)\n","Shape of X_test: (4128, 13)\n","Shape of y_train: (16512,)\n","Shape of y_test: (4128,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"e5abe4f4"},"source":["# Task\n","Train and evaluate a Linear Regression model using `X_train` and `y_train`, then predict on `X_train` and `X_test`, calculating and printing the RMSE for both training and testing sets, and the MAE for the testing set."]},{"cell_type":"markdown","metadata":{"id":"eeb230e0"},"source":["## Train and Evaluate Linear Regression\n","\n","### Subtask:\n","Instantiate and train a Linear Regression model using `X_train` and `y_train`. Make predictions on both training and testing sets, then calculate and print RMSE for both sets and MAE for the testing set.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5920f633","executionInfo":{"status":"ok","timestamp":1769006336408,"user_tz":-330,"elapsed":73,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"560ad06f-ece2-46b4-a337-e4668a7db368"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","\n","# 2. Instantiate a LinearRegression model. Initialize a dictionary named `results` to store metrics for all models.\n","lin_reg = LinearRegression()\n","results = {}\n","\n","# 3. Train the LinearRegression model using X_train and y_train.\n","lin_reg.fit(X_train, y_train)\n","\n","# 4. Make predictions on X_train and X_test, storing them as y_train_pred and y_test_pred, respectively.\n","y_train_pred = lin_reg.predict(X_train)\n","y_test_pred = lin_reg.predict(X_test)\n","\n","# 5. Calculate the Root Mean Squared Error (RMSE) for the training set\n","rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n","\n","# 6. Calculate the Root Mean Squared Error (RMSE) for the test set\n","rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n","\n","# 7. Calculate the Mean Absolute Error (MAE) for the test set\n","mae_test = mean_absolute_error(y_test, y_test_pred)\n","\n","# 8. Store these metrics in the `results` dictionary\n","results['Linear Regression'] = {\n","    'RMSE (Train)': rmse_train,\n","    'RMSE (Test)': rmse_test,\n","    'MAE (Test)': mae_test\n","}\n","\n","# 9. Print the calculated metrics for Linear Regression.\n","print(f\"Linear Regression Metrics:\")\n","print(f\"  RMSE (Train): {results['Linear Regression']['RMSE (Train)']:.2f}\")\n","print(f\"  RMSE (Test): {results['Linear Regression']['RMSE (Test)']:.2f}\")\n","print(f\"  MAE (Test): {results['Linear Regression']['MAE (Test)']:.2f}\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear Regression Metrics:\n","  RMSE (Train): 68433.94\n","  RMSE (Test): 70060.52\n","  MAE (Test): 50670.74\n"]}]},{"cell_type":"markdown","metadata":{"id":"039b9779"},"source":["## Train and Evaluate Ridge Regression\n","\n","### Subtask:\n","Instantiate and train a Ridge Regression model using `X_train` and `y_train`. Make predictions on both training and testing sets, then calculate and print RMSE for both sets and MAE for the testing set. Store the results in the `results` dictionary."]},{"cell_type":"markdown","metadata":{"id":"3f5d0135"},"source":["#### Instructions\n","1. Import `Ridge` from `sklearn.linear_model`.\n","2. Instantiate a `Ridge` model. (You can set `random_state=42` for reproducibility if desired, although Ridge regression is generally deterministic without it).\n","3. Train the `Ridge` model using `X_train` and `y_train`.\n","4. Make predictions on `X_train` and `X_test`, storing them as `y_train_pred_ridge` and `y_test_pred_ridge`, respectively.\n","5. Calculate the Root Mean Squared Error (RMSE) for the training set by taking the square root of `mean_squared_error` between `y_train` and `y_train_pred_ridge`. Store this as `rmse_train_ridge`.\n","6. Calculate the Root Mean Squared Error (RMSE) for the test set by taking the square root of `mean_squared_error` between `y_test` and `y_test_pred_ridge`. Store this as `rmse_test_ridge`.\n","7. Calculate the Mean Absolute Error (MAE) for the test set using `mean_absolute_error` between `y_test` and `y_test_pred_ridge`. Store this as `mae_test_ridge`.\n","8. Store these metrics in the `results` dictionary under the key 'Ridge Regression', using sub-keys 'RMSE (Train)', 'RMSE (Test)', and 'MAE (Test)'.\n","9. Print the calculated `RMSE (Train)`, `RMSE (Test)`, and `MAE (Test)` for Ridge Regression."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"930fd650","executionInfo":{"status":"ok","timestamp":1769006354686,"user_tz":-330,"elapsed":125,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"74d8d287-e204-435b-bd66-02b31e36ec5f"},"source":["from sklearn.linear_model import Ridge\n","\n","# 2. Instantiate a Ridge model.\n","ridge_reg = Ridge(random_state=42)\n","\n","# 3. Train the Ridge model using X_train and y_train.\n","ridge_reg.fit(X_train, y_train)\n","\n","# 4. Make predictions on X_train and X_test\n","y_train_pred_ridge = ridge_reg.predict(X_train)\n","y_test_pred_ridge = ridge_reg.predict(X_test)\n","\n","# 5. Calculate the Root Mean Squared Error (RMSE) for the training set\n","rmse_train_ridge = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))\n","\n","# 6. Calculate the Root Mean Squared Error (RMSE) for the test set\n","rmse_test_ridge = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))\n","\n","# 7. Calculate the Mean Absolute Error (MAE) for the test set\n","mae_test_ridge = mean_absolute_error(y_test, y_test_pred_ridge)\n","\n","# 8. Store these metrics in the `results` dictionary\n","results['Ridge Regression'] = {\n","    'RMSE (Train)': rmse_train_ridge,\n","    'RMSE (Test)': rmse_test_ridge,\n","    'MAE (Test)': mae_test_ridge\n","}\n","\n","# 9. Print the calculated metrics for Ridge Regression.\n","print(f\"Ridge Regression Metrics:\")\n","print(f\"  RMSE (Train): {results['Ridge Regression']['RMSE (Train)']:.2f}\")\n","print(f\"  RMSE (Test): {results['Ridge Regression']['RMSE (Test)']:.2f}\")\n","print(f\"  MAE (Test): {results['Ridge Regression']['MAE (Test)']:.2f}\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Ridge Regression Metrics:\n","  RMSE (Train): 68435.00\n","  RMSE (Test): 70067.35\n","  MAE (Test): 50677.17\n"]}]},{"cell_type":"markdown","metadata":{"id":"3ebd0ef5"},"source":["## Train and Evaluate Decision Tree Regressor\n","\n","### Subtask:\n","Instantiate and train a Decision Tree Regressor model using `X_train` and `y_train`. Make predictions on both training and testing sets, then calculate and print RMSE for both sets and MAE for the testing set. Store the results in the `results` dictionary."]},{"cell_type":"markdown","metadata":{"id":"03012927"},"source":["#### Instructions\n","1. Import `DecisionTreeRegressor` from `sklearn.tree`.\n","2. Instantiate a `DecisionTreeRegressor` model. Set `random_state=42` for reproducibility.\n","3. Train the `DecisionTreeRegressor` model using `X_train` and `y_train`.\n","4. Make predictions on `X_train` and `X_test`, storing them as `y_train_pred_dt` and `y_test_pred_dt`, respectively.\n","5. Calculate the Root Mean Squared Error (RMSE) for the training set by taking the square root of `mean_squared_error` between `y_train` and `y_train_pred_dt`. Store this as `rmse_train_dt`.\n","6. Calculate the Root Mean Squared Error (RMSE) for the test set by taking the square root of `mean_squared_error` between `y_test` and `y_test_pred_dt`. Store this as `rmse_test_dt`.\n","7. Calculate the Mean Absolute Error (MAE) for the test set using `mean_absolute_error` between `y_test` and `y_test_pred_dt`. Store this as `mae_test_dt`.\n","8. Store these metrics in the `results` dictionary under the key 'Decision Tree Regressor', using sub-keys 'RMSE (Train)', 'RMSE (Test)', and 'MAE (Test)'.\n","9. Print the calculated `RMSE (Train)`, `RMSE (Test)`, and `MAE (Test)` for Decision Tree Regressor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2965a80","executionInfo":{"status":"ok","timestamp":1769006369346,"user_tz":-330,"elapsed":453,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"21d47ddf-5964-4a9c-8cb1-fe56533ea482"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# 2. Instantiate a DecisionTreeRegressor model.\n","decision_tree_reg = DecisionTreeRegressor(random_state=42)\n","\n","# 3. Train the DecisionTreeRegressor model using X_train and y_train.\n","decision_tree_reg.fit(X_train, y_train)\n","\n","# 4. Make predictions on X_train and X_test\n","y_train_pred_dt = decision_tree_reg.predict(X_train)\n","y_test_pred_dt = decision_tree_reg.predict(X_test)\n","\n","# 5. Calculate the Root Mean Squared Error (RMSE) for the training set\n","rmse_train_dt = np.sqrt(mean_squared_error(y_train, y_train_pred_dt))\n","\n","# 6. Calculate the Root Mean Squared Error (RMSE) for the test set\n","rmse_test_dt = np.sqrt(mean_squared_error(y_test, y_test_pred_dt))\n","\n","# 7. Calculate the Mean Absolute Error (MAE) for the test set\n","mae_test_dt = mean_absolute_error(y_test, y_test_pred_dt)\n","\n","# 8. Store these metrics in the `results` dictionary\n","results['Decision Tree Regressor'] = {\n","    'RMSE (Train)': rmse_train_dt,\n","    'RMSE (Test)': rmse_test_dt,\n","    'MAE (Test)': mae_test_dt\n","}\n","\n","# 9. Print the calculated metrics for Decision Tree Regressor.\n","print(f\"Decision Tree Regressor Metrics:\")\n","print(f\"  RMSE (Train): {results['Decision Tree Regressor']['RMSE (Train)']:.2f}\")\n","print(f\"  RMSE (Test): {results['Decision Tree Regressor']['RMSE (Test)']:.2f}\")\n","print(f\"  MAE (Test): {results['Decision Tree Regressor']['MAE (Test)']:.2f}\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Regressor Metrics:\n","  RMSE (Train): 0.00\n","  RMSE (Test): 69116.03\n","  MAE (Test): 43600.67\n"]}]},{"cell_type":"markdown","metadata":{"id":"ed5021de"},"source":["**Reasoning**:\n","All three requested models have been trained and evaluated, and their metrics stored in the `results` dictionary. Now, I will display the `results` dictionary in a clear format to prepare for the final comparison and analysis as required by the main task.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c8ce35a","executionInfo":{"status":"ok","timestamp":1769006376610,"user_tz":-330,"elapsed":36,"user":{"displayName":"Madhura Suroshe","userId":"09991069889067260350"}},"outputId":"4ab205ee-3453-48c8-f3fd-4541a09aa1d2"},"source":["import pandas as pd\n","\n","# Display the results dictionary as a DataFrame for easy comparison\n","results_df = pd.DataFrame(results).T\n","print(\"\\nModel Performance Comparison:\")\n","print(results_df.round(2))\n"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Model Performance Comparison:\n","                         RMSE (Train)  RMSE (Test)  MAE (Test)\n","Linear Regression            68433.94     70060.52    50670.74\n","Ridge Regression             68435.00     70067.35    50677.17\n","Decision Tree Regressor          0.00     69116.03    43600.67\n"]}]},{"cell_type":"markdown","metadata":{"id":"fbf5f3ed"},"source":["## Model Comparison and Analysis\n","\n","**Performance Comparison:**\n","\n","Let's analyze the performance of the three models based on the calculated metrics:\n","\n","| Model                   | RMSE (Train) | RMSE (Test) | MAE (Test) |\n","|:------------------------|:-------------|:------------|:-----------|\n","| Linear Regression       | 68433.94     | 70060.52    | 50670.74   |\n","| Ridge Regression        | 68435.00     | 70067.35    | 50677.17   |\n","| Decision Tree Regressor | 0.00         | 69116.03    | 43600.67   |\n","\n","**Observations:**\n","\n","1.  **Linear Regression and Ridge Regression:**\n","    *   Both models show very similar performance metrics for both training and testing sets. This suggests that the regularization applied by Ridge Regression (with default alpha) had a minimal effect, likely because the dataset does not have strong multicollinearity or excessively large coefficients. Their RMSE (Train) and RMSE (Test) are close, indicating a reasonable fit to the data without significant overfitting or underfitting to the training set alone. The test RMSEs are slightly higher than training RMSEs, which is expected.\n","\n","2.  **Decision Tree Regressor:**\n","    *   **RMSE (Train): 0.00** This is a strong indicator of **overfitting**. A perfect RMSE on the training set means the model has learned the training data too well, essentially memorizing it, including noise. This typically happens with deep decision trees that are allowed to grow without restrictions (like `max_depth`).\n","    *   **RMSE (Test): 69116.03** While the training RMSE is 0, the test RMSE is comparable to, or slightly better than, Linear and Ridge Regression. This shows that despite severe overfitting to the training data, its generalization performance is not drastically worse than the linear models, and its MAE (Test) is even the best among the three. This could imply that there are some underlying patterns that the Decision Tree is capturing more effectively, but its lack of generalization on novel data points is masked by the overfitting.\n","    *   **MAE (Test): 43600.67** The Decision Tree Regressor achieved the lowest MAE on the test set, suggesting that on average, its predictions are closer to the actual values than the linear models, despite its high variance (as implied by the perfect training score).\n","\n","**Underfitting and Overfitting:**\n","\n","*   **Underfitting:** None of the models appear to be significantly underfitting. Linear and Ridge Regression show comparable train and test scores, suggesting they are capturing the general trends. If they were underfitting, both train and test scores would be high and likely similar, indicating the model is too simple to learn the patterns.\n","*   **Overfitting:** The Decision Tree Regressor exhibits severe **overfitting**. Its RMSE of 0.00 on the training set confirms that it has perfectly learned the training data, but this performance does not generalize well to unseen data, even if its test RMSE is competitive. The significant gap between its training RMSE and test RMSE (or the perfect train RMSE vs. a non-perfect test RMSE) is the hallmark of overfitting.\n","\n","**Relevant Real-World Machine Learning Issue: Interpretability vs. Performance Trade-off**\n","\n","This analysis highlights the classic trade-off between model interpretability and predictive performance, often tied to complexity. Linear and Ridge Regression are highly interpretable models; their coefficients directly indicate the impact of each feature on the house price. However, they might struggle to capture complex non-linear relationships. Decision Trees, especially when unconstrained, can capture very complex relationships, leading to high performance on the training data. However, their interpretability can decrease with depth, and they are prone to overfitting. In a real-world scenario for house price prediction, a highly interpretable model might be preferred by stakeholders (e.g., real estate agents, appraisers) to understand **why** a house is priced a certain way, even if a slightly less interpretable model (like a Random Forest or Gradient Boosting, which are ensembles of Decision Trees) offers marginally better predictive accuracy. The choice often depends on the specific business needs: is it about accurate prediction *or* understanding the driving factors?"]},{"cell_type":"markdown","metadata":{"id":"e27e30f7"},"source":["## Final Task\n","\n","### Subtask:\n","Summarize the performance metrics for all three models (Linear Regression, Ridge Regression, and Decision Tree Regressor), including RMSE (Train), RMSE (Test), and MAE (Test) for each. This will allow for a comparative analysis of their performance.\n"]},{"cell_type":"markdown","metadata":{"id":"8b073c58"},"source":["## Summary:\n","\n","### Q&A\n","The task was to summarize the performance metrics for Linear Regression, Ridge Regression, and Decision Tree Regressor models, including RMSE (Train), RMSE (Test), and MAE (Test) for each, to facilitate a comparative analysis.\n","\n","The performance metrics for the models are summarized as follows:\n","\n","| Model                   | RMSE (Train) | RMSE (Test) | MAE (Test) |\n","| :---------------------- | :----------- | :---------- | :--------- |\n","| Linear Regression       | \\$68433.94   | \\$70060.52  | \\$50670.74 |\n","| Ridge Regression        | \\$68435.00   | \\$70067.35  | \\$50677.17 |\n","| Decision Tree Regressor | \\$0.00       | \\$69116.03  | \\$43600.67 |\n","\n","### Data Analysis Key Findings\n","\n","*   **Linear and Ridge Regression Performance:** Both Linear Regression and Ridge Regression models exhibited very similar performance, with RMSE (Train) at \\$68433.94 and \\$68435.00 respectively, and RMSE (Test) at \\$70060.52 and \\$70067.35 respectively. Their MAE (Test) were also comparable at \\$50670.74 and \\$50677.17. This suggests that the regularization in Ridge Regression had a minimal impact, possibly due to the dataset's characteristics.\n","*   **Decision Tree Regressor Overfitting:** The Decision Tree Regressor showed clear signs of severe overfitting, achieving a perfect RMSE (Train) of \\$0.00. This indicates it memorized the training data entirely.\n","*   **Decision Tree Regressor Test Performance:** Despite overfitting on the training data, the Decision Tree Regressor's RMSE (Test) was competitive at \\$69116.03, and it achieved the lowest MAE (Test) among all models at \\$43600.67, suggesting better average absolute predictions on unseen data.\n","*   **No Significant Underfitting:** None of the models appeared to be significantly underfitting, as indicated by reasonably close train and test scores for the linear models and competitive test scores for the Decision Tree."]}]}